{"cells":[{"cell_type":"markdown","source":["# Fine-Tuning ChatGPT on Domain-Specific Content \n","This program demonstrates how to fine tune and use openAI's ChatGPT language model to answer questions in specific domain areas.  \n","\n","The sample content used is drawn from the 2023 investment outlook summaries posted on the websites of \n","Morgan Stanley [here](https://www.morganstanley.com/ideas/global-investment-strategy-outlook-2023), \n","JPMorgan [here](https://www.jpmorgan.com/insights/research/market-outlook) and \n","Goldman Sachs [here](https://www.goldmansachs.com/insights/pages/gs-research/macro-outlook-2023-this-cycle-is-different/report.pdf).  \n","\n","The base ChatGPT model (GPT-3) is first fine-tuned on the sample content.  The new model is able to answer questions on the new content -- but only vaguely, at a very high level. \n","\n","Next, selected content is appended to each prompt as context before it fed to  the fine-tuned model. Specifically, an interface asks a user for a question about the banks' investment outlooks.  The program compares the user's query with the domain content to identify the most useful sections of text. The program answers the question by using the fine-tuned model's powerful underlying capabilities while referencing the specific context supplied in the prompt.\n","\n","For a detailed discussion, see [\"Leveraging ChatGPT for\n","Business and Organizational Purposes\"](https://github.com/robjm16/domain_specific_ChatGPT/blob/main/DOMAIN_SPECIFIC_CHATGPT.md)."],"metadata":{"id":"pwXngZcFdm0o"}},{"cell_type":"markdown","source":["##1.Install Libraries "],"metadata":{"id":"wE71JnURemJx"}},{"cell_type":"code","source":["! pip install openai \n","! pip install transformers \n","! pip install gradio\n","! pip install PyPDF2\n","! pip install python-docx\n","! pip install pandas"],"metadata":{"id":"55GBv-r7-TdX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2.Imports "],"metadata":{"id":"MldSfQqt-0JW"}},{"cell_type":"code","source":["import docx\n","import pandas as pd\n","import numpy as np\n","import json \n","import openai\n","import gradio as gr\n","import pickle\n","import ast\n","import os\n","from transformers import GPT2TokenizerFast\n","from sklearn.model_selection import train_test_split # only if using validation file "],"metadata":{"id":"p73n3KA8-S_t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##3.Variables"],"metadata":{"id":"bD2MldPAe_gt"}},{"cell_type":"code","source":["USE_INTERFACE = True  # Change to False if you want to run the code without the Gradio interface, and instead see a single pre-supplied question \n","filepath = 'investment_outlook_2023.docx' # Path to document containing domain content.  \n","# emb_filepath = 'PATH HERE'  # Path to document containing saved content embeddings, if applicable \n","COMPLETIONS_MODEL = \"text-davinci-003\"  \n","api_key = 'YOUR OPENAI KEY HERE'\n","os.environ['API_KEY'] = api_key                                              \n","openai.api_key = os.environ[\"API_KEY\"]\n","MODEL_NAME = \"curie\"\n","DOC_EMBEDDINGS_MODEL = f\"text-search-{MODEL_NAME}-doc-001\"\n","QUERY_EMBEDDINGS_MODEL = f\"text-search-{MODEL_NAME}-query-001\"\n","MAX_SECTION_LEN =1100  # The API limits total tokens -- for the prompt containing the wuestion and domain-specific content and the answer -- to 2048 tokens, or about 1500 words.  \n","SEPARATOR = \"\\n* \"  # A string called SEPARATOR is defined as the newline character followed by an asterisk and a space. This string will be used as a separator between different pieces of text.\n","tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n","separator_len = len(tokenizer.tokenize(SEPARATOR))\n","COMPLETIONS_API_PARAMS = {\n","    # We use temperature of 0.0 because it gives the most predictable, factual answer.\n","    \"temperature\": 0.0,\n","    \"max_tokens\": 300,\n","    \"model\": COMPLETIONS_MODEL,  \n","    \"stop\":[\".###\"]\n","}\n"],"metadata":{"id":"SGTDdek1-S7k"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 4.Functions"],"metadata":{"id":"iL_FM8aQ-8ac"}},{"cell_type":"code","source":["def load_text(filepath):\n","  \"\"\"\n","  Loads a Microsoft Word document and returns a DataFrame containing the text of each paragraph in the document.\n","\n","  Input:\n","    filepath (str): the filepath to the Microsoft Word document.\n","    \n","  Returns:\n","    df (pandas.DataFrame): a DataFrame containing the 'content' column with the text of each paragraph in the document.\n","  \"\"\"\n","  # Open the Word document\n","  doc = docx.Document(filepath)\n","\n","  # Create an empty pandas DataFrame\n","  df = pd.DataFrame()\n","\n","  # Iterate through the paragraphs in the document and add each to the df\n","  for i, p in enumerate(doc.paragraphs):\n","\n","      # Add the paragraph text [and index to the DataFrame]    \n","      df.loc[i, 'content'] = p.text\n","      # df.loc[i, 'paragraph_index'] = i\n","\n","  # Delete empty paragraphs\n","  df['content'] = df['content'].replace('', np.nan)\n","  df = df.dropna(axis=0, subset=['content']).reset_index(drop=True)\n","\n","  return df\n","    \n","def count_tokens(row):\n","    \"\"\"count the number of tokens in a string\"\"\"\n","    return len(tokenizer.encode(row))\n","\n","def truncate_text(df):\n","    \"\"\"\n","    Truncates the text in the 'content' column of the input DataFrame if the number of tokens \n","    in the text exceeds a specified maximum number. It will set the truncated text and the \n","    number of tokens in the 'content' and 'tokens' columns, respectively.\n","\n","    Input:\n","    df (pandas.DataFrame): a DataFrame containing the 'content' column\n","\n","    Returns:\n","    df (pandas.DataFrame): the input DataFrame with modified 'content' and 'tokens' columns.\n","\n","    \"\"\"\n","    for i in range(len(df)):\n","        if df['tokens'][i] > 590:\n","            text = df['content'][i]\n","            tokens = tokenizer.encode(text)\n","            truncated_tokens = tokens[:590]\n","            truncated_text = tokenizer.decode(truncated_tokens)\n","            df.at[i, 'content'] = truncated_text\n","            df.at[i, 'tokens'] = len(truncated_tokens)\n","    return df\n","\n"," \n","def get_embedding(text, model): \n","    \"\"\"\n","    Generates an embedding for the given text using the specified OpenAI model.\n","    \n","    Args:\n","        text (str): The text for which to generate an embedding.\n","        model (str): The name of the OpenAI model to use for generating the embedding.\n","    \n","    Returns:\n","        numpy.ndarray: The embedding for the given text.\n","    \"\"\"\n","    result = openai.Embedding.create(\n","      model=model,\n","      input=[text]\n","    )\n","    return result[\"data\"][0][\"embedding\"]\n","\n","def get_doc_embedding(text):\n","    \"\"\"\n","    Generates an embedding for the given text using the OpenAI document embeddings model.\n","    \n","    Args:\n","        text (str): The text for which to generate an embedding.\n","    \n","    Returns:\n","        numpy.ndarray: The embedding for the given text.\n","    \"\"\"\n","    return get_embedding(text, DOC_EMBEDDINGS_MODEL)\n","\n","def get_query_embedding(text):\n","   \"\"\"\n","    Generates an embedding for the given text using the OpenAI query embeddings model.\n","    \n","    Args:\n","        text (str): The text for which to generate an embedding.\n","    \n","    Returns:\n","        numpy.ndarray: The embedding for the given text.\n","    \"\"\"\n","   return get_embedding(text, QUERY_EMBEDDINGS_MODEL)\n","\n","def compute_doc_embeddings(df): \n","     \"\"\"\n","    Generate embeddings for each row in a Pandas DataFrame using the OpenAI document embeddings model.\n","    \n","    Args:\n","        df (pandas.DataFrame): The DataFrame for which to generate embeddings.\n","    \n","    Returns:\n","        dict: A dictionary that maps the embedding vectors to the indices of the rows that they correspond to.\n","    \"\"\"\n","     return {\n","        idx: get_doc_embedding(r.content.replace(\"\\n\", \" \")) for idx, r in df.iterrows() # r here refers to each row \n","   }\n","\n","def load_embeddings(fname): \n","    \"\"\"\n","    Load document embeddings and their keys from a CSV file.  Only if embeddings are pre-loaded.\n","    \n","    Args:\n","        fname (str): The path to the CSV file. The file must have exactly these named columns: \n","            \"title\", \"heading\", \"0\", \"1\", ... up to the length of the embedding vectors.\n","    \n","    Returns:\n","        dict: A dictionary that maps the embedding vectors to tuples of the form (title, heading).\n","    \"\"\"\n","    \n","    df = pd.read_csv(fname, header=0)\n","    max_dim = max([int(c) for c in df.columns if c != \"title\" and c != \"heading\"])\n","    return {\n","           (r.title, r.heading): [r[str(i)] for i in range(max_dim + 1)] for _, r in df.iterrows()\n","    }\n","\n","def vector_similarity(x, y):\n","    \"\"\"\n","    Calculate the similarity between two vectors using dot product.\n","    \n","    Args:\n","        x (iterable): The first vector.\n","        y (iterable): The second vector.\n","    \n","    Returns:\n","        float: The dot product of the two vectors.\n","    \"\"\"\n","    return np.dot(np.array(x), np.array(y))\n","\n","def order_document_sections_by_query_similarity(query, contexts):  #  CHANGED FROM (query, contexts)???????????????????????????\n","    \"\"\"\n","    Find the query embedding for the given query, and compare it against all of the pre-calculated document embeddings\n","    to find the most relevant sections. \n","    \n","    Args:\n","        query (str): The query for which to find relevant document sections.\n","        contexts (dict): A dictionary mapping document embeddings to their indices.\n","      \n","    Returns:\n","        list: A list of tuples, each containing the similarity score and index of a document section, sorted in descending\n","        order of relevance.\n","    \"\"\"\n","    query_embedding = get_query_embedding(query)\n","    print(\"GETTING DOC SIMILARIITIES.........\")  # FOR TESTING PURPOSES\n","    document_similarities = sorted([(vector_similarity(query_embedding, doc_embedding), doc_index) \\\n","                                    for doc_index, doc_embedding in contexts.items()], \\\n","                                    reverse=True)\n","    print(\"FINISHED DOC SIMILARITIES..............\")  # FOR TESTING PURPOSES\n","    \n","    return document_similarities\n","    \n","def construct_prompt(question, context_embeddings, df):\n","    \"\"\"\n","    Construct a prompt for answering a question using the most relevant document sections.\n","    \n","    Args:\n","      question (str): The question to answer.\n","      context_embeddings (dict): A dictionary mapping document embeddings to their indices.\n","      df (pandas.DataFrame): A DataFrame containing the document sections.\n","    \n","    Returns:\n","      str: The prompt, including the question and the relevant context.\n","    \"\"\"\n","    most_relevant_document_sections = order_document_sections_by_query_similarity(question, context_embeddings)\n","    \n","    chosen_sections = []\n","    chosen_sections_len = 0\n","    chosen_sections_indexes = []\n","     \n","    for _, section_index in most_relevant_document_sections:\n","        # Add contexts until we run out of space.        \n","        document_section = df.loc[section_index]\n","        \n","        chosen_sections_len += document_section.tokens + separator_len  # Note that \"token\" column is used here \n","        if chosen_sections_len > MAX_SECTION_LEN:\n","            break\n","            \n","        chosen_sections.append(SEPARATOR + document_section.content.replace(\"\\n\", \" \")) # Note that 'content\" column is used here \n","        chosen_sections_indexes.append(str(section_index))\n","            \n","    # Useful diagnostic information  -- FOR TESTING PURPOSES\n","    print(f\"Selected {len(chosen_sections)} document sections:\")\n","    print(\"\\n\".join(chosen_sections_indexes))\n","    \n","    header = \"\"\"Given the following context, answer the question as truthfully as possible, and if the answer is not contained within the context below, say \"Sorry, I don't know.\"\\n\\nContext:\\n\"\"\"\n","\n","    full_prompt = header + \"\".join(chosen_sections) + \"\\n\\nQuestion: \" + question + \"\\n\\n###\\n\\n\"\n","\n","    print(full_prompt) # FOR TESTING PURPOSES\n","\n","    return full_prompt\n","    \n","\n","def answer_query_with_context(\n","    query,\n","    df,\n","    document_embeddings,\n","    show_prompt: bool = False):\n","    prompt = construct_prompt(\n","        query,\n","        document_embeddings,\n","        df\n","    )\n","    \"\"\"\n","    Answer a query using relevant context from a DataFrame.\n","    \n","    Args:\n","        query (str): The query to answer.\n","        df (pandas.DataFrame): A DataFrame containing the document sections.\n","        document_embeddings (dict): A dictionary mapping document embeddings to their indices.\n","        show_prompt (bool, optional): If `True`, print the prompt before generating a response.\n","    \n","    Returns:\n","        str: The generated response to the query.\n","    \"\"\"   \n","    # print(\"LINE 232..............\")  # FOR TESTING PURPOSES\n","\n","\n","\n","    if show_prompt:\n","        print(prompt)\n","\n","    response = openai.Completion.create(\n","                prompt=prompt,\n","                **COMPLETIONS_API_PARAMS\n","            )\n","\n","    return response[\"choices\"][0][\"text\"].strip(\" \\n\")\n","\n","def get_questions(context):\n","    \"\"\"\n","    get_questions(context) is a function that takes in a string of text(context) \n","    as an argument and returns a string of questions generated based on the context \n","    using the OpenAI API. The function uses the \"text-davinci-001\" engine, the \n","    prompt is constructed by combining the context and the string \"Write questions \n","    based on the text below\\n\\nText: {context}\\n\\nQuestions:\\n1.\"  \n","    The temperature, max_tokens, top_p, frequency_penalty, presence_penalty all set \n","    to 0, and stop is set to \"\\n\\n\"\n","    If there is any exception, the function will return an empty string.\n","    \"\"\"\n","    try:\n","        response = openai.Completion.create(\n","            engine=\"text-davinci-001\",\n","            prompt=f\"Write questions based on the text below\\n\\nText: {context}\\n\\nQuestions:\\n1.\",\n","            temperature=0,\n","            max_tokens=257,\n","            top_p=1,\n","            frequency_penalty=0,\n","            presence_penalty=0,\n","            stop=[\"\\n\\n\"]\n","        )\n","        # print(response)  # FOR TESTING PURPOSES\n","        # print(response['choices'][0]['text'])  # FOR TESTING PURPOSES\n","        return response['choices'][0]['text']\n","    except:\n","        return \"\"\n","\n","def get_answers(row):\n","    \"\"\"\n","    get_answers(row) is a function that takes in a row of dataframe\n","    and returns a string of answers generated based on the questions and context \n","    in the dataframe using the OpenAI API.\n","    The function uses the \"text-davinci-001\" engine, the prompt is constructed by\n","    combining the context and the questions in the dataframe.\n","    The temperature, max_tokens, top_p, frequency_penalty, presence_penalty all \n","    set to 0.\n","    If there is any exception, the function will print the exception and return \n","    an empty string.\n","  \"\"\"\n","    try:\n","        response = openai.Completion.create(\n","            engine=\"text-davinci-001\",\n","            prompt=f\"Write questions based on the text below\\n\\nText: {row.context}\\n\\nQuestions:\\n{row.questions}\\n\\nAnswers:\\n1.\",\n","            temperature=0,\n","            max_tokens=257,\n","            top_p=1,\n","            frequency_penalty=0,\n","            presence_penalty=0\n","        )\n","        return response['choices'][0]['text']\n","    except Exception as e:\n","        print (e)\n","        return \"\"\n","\n"],"metadata":{"id":"uhlSmsYj-S0v"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##5.Fine Tune the Model "],"metadata":{"id":"bsMjT5AW_R6B"}},{"cell_type":"markdown","source":["###5A. Prepare the Master Dataframe \n","Load the content, count tokens, generate fine-tuning questions and truncate long content as needed \n","\n"],"metadata":{"id":"ZM7SB03TBlL8"}},{"cell_type":"code","source":["# Load the text into dataframe \n","df = load_text(filepath)\n","# print(df.head()) # For Testing Purposes.................\n","\n","# Count the tokens; used to size sections in potentially truncating text df = df.copy()    \n","df['tokens'] = df['content'].apply(count_tokens)\n","\n","# print(df.head(10))   # For Testing Purposes................. \n","# print(df['content'][3])   # For Testing Purposes.................\n","\n","df['context'] = df['content']  # Leaving content column as is, for future reference \n","df['questions']= df['context'].apply(get_questions) # Generates questions for each section\n","df['questions'] = \"1.\" + df.questions  # Adds number to first question\n","# print(df[['questions']].values[0][0]) # For Testing Purposes.................\n","\n","# Call the truncate_text function on the dataframe  \n","df = df.copy()    \n","df = truncate_text(df)"],"metadata":{"id":"_AeZWH9v-S5e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Add answers (to the questions), paragraph numbers and embeddings of the content to the dataframe.  \n","NOTE:  May need to wait >1 min to run next cell in order to stay under free-of-charge usage limits.  \n"],"metadata":{"id":"wS-lJYDk2DpY"}},{"cell_type":"code","source":["df['answers']= df.apply(get_answers, axis=1)\n","df['answers'] = \"1.\" + df.answers  # Adds first number to answers\n","df = df.dropna().reset_index().drop('index',axis=1)\n","print(df[['answers']].values[0][0]) \n","# df.tail()  # For testing purposes.....................\n","# Add paragraph number column for optional use later in generating adversarial context/answer\n","for i, row in enumerate(df.iterrows()):\n","    df.loc[i, \"paragraph_number\"] = i\n","# Add embeddings of \"context\" column text to df, to allow repeated use \n","document_embeddings = compute_doc_embeddings(df)\n","df = df.assign(embeddings=df.index.map(document_embeddings))\n","df.head(3) # For testing purposes ..............................................."],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":295},"id":"027MbwPr-SrB","executionInfo":{"status":"ok","timestamp":1675171016267,"user_tz":300,"elapsed":55435,"user":{"displayName":"Bob McKee","userId":"09961974450561244966"}},"outputId":"d13de5fc-b093-4627-ad43-e58c74d7925b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1. The main difference between the market conditions in 2022 and 2023 is that the market will be in a more stable environment with slower growth and lower inflation.\n","2. The outlook for income investing in 2023 is positive, with good opportunities for investors to find income-producing assets.\n","3. The main risks to the market outlook in 2023 are potential reversals of the current market trends.\n"]},{"output_type":"execute_result","data":{"text/plain":["                                             content  tokens  \\\n","0  Morgan Stanley says:  In an environment of slo...     143   \n","1  Morgan Stanley says:  Bonds—the biggest losers...     124   \n","2  Morgan Stanley says: Other key takeaways from ...     142   \n","\n","                                             context  \\\n","0  Morgan Stanley says:  In an environment of slo...   \n","1  Morgan Stanley says:  Bonds—the biggest losers...   \n","2  Morgan Stanley says: Other key takeaways from ...   \n","\n","                                           questions  \\\n","0  1. What is the main difference between the mar...   \n","1  1. What are the global macro trends that Morga...   \n","2  1. What is the main reason for the predicted d...   \n","\n","                                             answers  paragraph_number  \\\n","0  1. The main difference between the market cond...               0.0   \n","1  1. Morgan Stanley believes that global macro t...               1.0   \n","2  1. The main reason for the predicted decline i...               2.0   \n","\n","                                          embeddings  \n","0  [0.028803126886487007, -0.007771830074489117, ...  \n","1  [0.010308968834578991, -0.00835722591727972, -...  \n","2  [0.013922976329922676, -0.004230298567563295, ...  "],"text/html":["\n","  <div id=\"df-31fd71eb-7992-4ff3-beb5-79c9db655486\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>content</th>\n","      <th>tokens</th>\n","      <th>context</th>\n","      <th>questions</th>\n","      <th>answers</th>\n","      <th>paragraph_number</th>\n","      <th>embeddings</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Morgan Stanley says:  In an environment of slo...</td>\n","      <td>143</td>\n","      <td>Morgan Stanley says:  In an environment of slo...</td>\n","      <td>1. What is the main difference between the mar...</td>\n","      <td>1. The main difference between the market cond...</td>\n","      <td>0.0</td>\n","      <td>[0.028803126886487007, -0.007771830074489117, ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Morgan Stanley says:  Bonds—the biggest losers...</td>\n","      <td>124</td>\n","      <td>Morgan Stanley says:  Bonds—the biggest losers...</td>\n","      <td>1. What are the global macro trends that Morga...</td>\n","      <td>1. Morgan Stanley believes that global macro t...</td>\n","      <td>1.0</td>\n","      <td>[0.010308968834578991, -0.00835722591727972, -...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Morgan Stanley says: Other key takeaways from ...</td>\n","      <td>142</td>\n","      <td>Morgan Stanley says: Other key takeaways from ...</td>\n","      <td>1. What is the main reason for the predicted d...</td>\n","      <td>1. The main reason for the predicted decline i...</td>\n","      <td>2.0</td>\n","      <td>[0.013922976329922676, -0.004230298567563295, ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-31fd71eb-7992-4ff3-beb5-79c9db655486')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-31fd71eb-7992-4ff3-beb5-79c9db655486 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-31fd71eb-7992-4ff3-beb5-79c9db655486');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["# Save df\n","df.to_csv('invest_outlook_2023.csv', index=False)\n","df = pd.read_csv('invest_outlook_2023.csv')\n","# df.head(3) # For testing purposes ..............................................."],"metadata":{"id":"TnnSZKfry2LW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###5B. Fine Tuning "],"metadata":{"id":"yYOIWHJpBgHw"}},{"cell_type":"markdown","source":["Create new dataframe with one question per row, create prompts, further process prompts/completions per OpenAI instructions"],"metadata":{"id":"N__iESNdkCY4"}},{"cell_type":"code","source":["# Create a new dataframe with one question and answer per row\n","expanded_df = pd.DataFrame(columns=df.columns)\n","for i, row in df.iterrows():\n","    questions = row['questions'].split(\"\\n\")\n","    answers = row['answers'].split(\"\\n\")\n","    for j in range(len(questions)):\n","        if j < len(questions) and j < len(answers):\n","            new_row = {'paragraph_number': row['paragraph_number'],\\\n","                       'content': row['content'], 'tokens': row['tokens'], \\\n","                       'context': row['context'], 'embeddings': row['embeddings'],\\\n","                       'questions': questions[j], 'answers': answers[j]}\n","            expanded_df = expanded_df.append(new_row, ignore_index=True)\n","\n","# Label questions \"original\" to distinguish from optional adversarial examples, if added \n","expanded_df[\"label\"] = \"original\"\n","expanded_df.rename(columns={'answers': 'completion'}, inplace=True)\n","\n","# Remove question/completion numbers\n","expanded_df['questions'] = expanded_df['questions'].str[2:].str.strip()\n","expanded_df['completion'] = expanded_df['completion'].str[2:].str.strip()\n","\n","# Create prompts\n","expanded_df[\"prompt\"] = expanded_df.apply(lambda row: f\"Context: {row['context']}\\nQuestion: {row['questions']} \", axis=1) \n","\n","# Add whitespace to start of completion and unqiue identfier to end, per OpenAI\n","expanded_df['completion'] = expanded_df['completion'].str.ljust(1)\n","expanded_df['prompt'] = expanded_df['prompt'] + \"\\n\\n###\\n\\n\"\n","expanded_df['completion'] = ' ' + expanded_df['completion'] + \"###\"\n","\n","expanded_df.head(2) # For testing only ................................."],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":191},"id":"FRTxyC6W-0Zz","executionInfo":{"status":"ok","timestamp":1675172125303,"user_tz":300,"elapsed":911,"user":{"displayName":"Bob McKee","userId":"09961974450561244966"}},"outputId":"099c1241-7e15-4df8-cf7f-16b43f805738"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                             content tokens  \\\n","0  Morgan Stanley says:  In an environment of slo...    143   \n","1  Morgan Stanley says:  In an environment of slo...    143   \n","\n","                                             context  \\\n","0  Morgan Stanley says:  In an environment of slo...   \n","1  Morgan Stanley says:  In an environment of slo...   \n","\n","                                           questions  \\\n","0  What is the main difference between the market...   \n","1  What is the outlook for income investing in 2023?   \n","\n","                                          completion  paragraph_number  \\\n","0   The main difference between the market condit...               0.0   \n","1   The outlook for income investing in 2023 is p...               0.0   \n","\n","                                          embeddings     label  \\\n","0  [0.028803126886487007, -0.007771830074489117, ...  original   \n","1  [0.028803126886487007, -0.007771830074489117, ...  original   \n","\n","                                              prompt  \n","0  Context: Morgan Stanley says:  In an environme...  \n","1  Context: Morgan Stanley says:  In an environme...  "],"text/html":["\n","  <div id=\"df-cf89154a-ed89-46dd-8a78-b1dc63f90c04\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>content</th>\n","      <th>tokens</th>\n","      <th>context</th>\n","      <th>questions</th>\n","      <th>completion</th>\n","      <th>paragraph_number</th>\n","      <th>embeddings</th>\n","      <th>label</th>\n","      <th>prompt</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Morgan Stanley says:  In an environment of slo...</td>\n","      <td>143</td>\n","      <td>Morgan Stanley says:  In an environment of slo...</td>\n","      <td>What is the main difference between the market...</td>\n","      <td>The main difference between the market condit...</td>\n","      <td>0.0</td>\n","      <td>[0.028803126886487007, -0.007771830074489117, ...</td>\n","      <td>original</td>\n","      <td>Context: Morgan Stanley says:  In an environme...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Morgan Stanley says:  In an environment of slo...</td>\n","      <td>143</td>\n","      <td>Morgan Stanley says:  In an environment of slo...</td>\n","      <td>What is the outlook for income investing in 2023?</td>\n","      <td>The outlook for income investing in 2023 is p...</td>\n","      <td>0.0</td>\n","      <td>[0.028803126886487007, -0.007771830074489117, ...</td>\n","      <td>original</td>\n","      <td>Context: Morgan Stanley says:  In an environme...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cf89154a-ed89-46dd-8a78-b1dc63f90c04')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-cf89154a-ed89-46dd-8a78-b1dc63f90c04 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-cf89154a-ed89-46dd-8a78-b1dc63f90c04');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","source":["Extract columns for fine tuning, create single line json file and fine tune via the API "],"metadata":{"id":"wCNMxqKCkg1-"}},{"cell_type":"code","source":["# Extract columns needed for fine tuning  \n","temp_df=expanded_df[[\"prompt\",\"completion\"]].copy()\n","\n","# Writes prompts/completions to jsonl (single lines)\n","df_train_list = temp_df.to_dict(orient='records')\n","with open('df_train3.jsonl', 'w', encoding='utf-8') as outfile:\n","    for row in df_train_list:\n","        outfile.write(json.dumps(row, ensure_ascii=False))\n","        outfile.write('\\n')\n","        # print(row)  # For testing purposes............................\n","\n","df_train_list[0] # For testing purposes.......................................\n","\n","# # If using validation file\n","# # Split the expanded dataframe into training and testing sets\n","# train_df, test_df = train_test_split(temp_df, test_size=0.2)\n","\n","# train_df = train_df.reset_index(drop=True)\n","# test_df = test_df.reset_index(drop=True)\n","\n","# Create (optional) adversarial questions; train dataset only\n","\n","# for i in range(1, len(train_df), 15):\n","#     if i < len(train_df):\n","#         # Randomly select a question from another paragraph\n","#         adversarial_question = train_df[train_df['paragraph_number'] != train_df.loc[i, 'paragraph_number']].sample(1)\n","#         # Replace the question with the adversarial value\n","#         train_df.loc[i, 'completion'] = adversarial_question['completion'].item()\n","#         train_df.loc[i, 'label'] = \"adversarial question\"\n","#     else:\n","#         break\n","\n","# for i in range(7, len(train_df), 15):\n","#     if i < len(train_df):\n","#         # Randomly select a context from another paragraph\n","#         adversarial_context = train_df[train_df['paragraph_number'] != train_df.loc[i, 'paragraph_number']].sample(1)\n","#         # Replace the context with the adversarial value\n","#         train_df.loc[i, 'context'] = adversarial_context['context'].item()\n","#         train_df.loc[i, 'label'] = \"adversarial context\"\n","#     else:\n","#         break\n","\n","# If separate train and test datasets \n","# df_train_list = train_df.to_dict(orient='records')\n","# with open('df_train2.jsonl', 'w', encoding='utf-8') as outfile:\n","#     for row in df_train_list:\n","#         outfile.write(json.dumps(row, ensure_ascii=False))\n","#         outfile.write('\\n')\n","#         print(row)  # For testing purposes\n","\n","# df_test_list = test_df.to_dict(orient='records')\n","# with open('df_test2.jsonl', 'w', encoding='utf-8') as outfile:\n","#     for row in df_test_list:\n","#         outfile.write(json.dumps(row, ensure_ascii=False))\n","#         outfile.write('\\n')\n","#         print(row)  # For testing purposes"],"metadata":{"id":"6x2JWFM3JHeI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1675172132749,"user_tz":300,"elapsed":186,"user":{"displayName":"Bob McKee","userId":"09961974450561244966"}},"outputId":"da5ccd57-127f-4d0a-b8fe-2dace218a765"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'prompt': 'Context: Morgan Stanley says:  In an environment of slow growth, lower inflation and new monetary policies, expect 2023 to have upside for bonds, defensive stocks and emerging markets. Investors may find themselves a bit whiplashed in 2023 as inflation and some of this year’s other dominant market trends fully reverse themselves, according to the 2023 Strategy Outlook from Morgan Stanley Research.  “For markets, this presents a\\xa0very\\xa0different backdrop than 2022, which was marked by resilient growth, high inflation and hawkish policy,” says Andrew Sheets, Chief Cross-Asset Strategist for Morgan Stanley Research. “Overall, 2023 will be a good year for income investing.” \\nQuestion: What is the main difference between the market conditions in 2022 and 2023? \\n\\n###\\n\\n',\n"," 'completion': ' The main difference between the market conditions in 2022 and 2023 is that the market will be in a more stable environment with slower growth and lower inflation.###'}"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["# Prepare the data in the JSONL file for fine-tuning\n","!openai tools fine_tunes.prepare_data -f df_train3.jsonl -q "],"metadata":{"id":"60AuUT6f8EFN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create the fine tuning \n","!openai api fine_tunes.create -t \"df_train3.jsonl\" "],"metadata":{"id":"W9X1jH_mGdDF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Load the fine-tuned model and test on a few questions"],"metadata":{"id":"e1DQhsXKl7rb"}},{"cell_type":"code","source":["# Test\n","question = ['Why is inflation so high?', 'What is the outlook for oil?', 'What does JPMorgan think about 2023?', 'What is the view on emerging markets?']\n","ft_model = 'ada:ft-openai-2021-07-30-12-26-20' # Example fine tune name only; OpenAI API provides proprietary model name after fine tuning \n","result = openai.Completion.create(model=ft_model, prompt=question[1] + '\\n\\n###\\n\\n', max_tokens=120, temperature=0, stop=[\".###\"]) # To test from test dataset use \"df_test_list['prompt'][0] + \" instead of \"question\" \n","print(result['choices'][0]['text'])"],"metadata":{"id":"FSLtA4YVOpnf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1675172151273,"user_tz":300,"elapsed":2681,"user":{"displayName":"Bob McKee","userId":"09961974450561244966"}},"outputId":"ef8c58a8-eff1-48a7-8883-fc5ca5919fee"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[" The outlook for oil is positive\n"]}]},{"cell_type":"markdown","source":["### 5C. Fine-Tuned Model with Context Added to Prompts  \n","After testing, it is clear that fine tuning on domain-specific content enables the new model to answer questions on that new knowledge base -- but the answers answers are less robust than when injecting specific context into a prompt.  \n","\n","Here, specific context is added to the prompts of the fine tuned model. Embeddings of the question and the knowledge base are used to extract the contexts that most directly fit the question, and those contexts are added to the prompt.   "],"metadata":{"id":"HIU3H8DhkClS"}},{"cell_type":"markdown","source":["If picking up previously fined tune model, change model name in completion paramters at top "],"metadata":{"id":"HapWO0-fqvPA"}},{"cell_type":"code","source":["df=df.reset_index()\n","df_excerpt = df[['content', 'tokens']].copy()\n","df['embeddings'] = df['embeddings'].apply(lambda x: [float(i) for i in ast.literal_eval(x)]) #Changes string to float\n","# # Create dictionary of embeddings, by row of df\n","doc_embeddings = df.set_index('index').to_dict()['embeddings']"],"metadata":{"id":"YvHoge8AB6YO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Launch the Q/A interface.  \n","NOTE: Additional information may be printed for validation purposes"],"metadata":{"id":"NyfWuI3aoM9p"}},{"cell_type":"code","source":["if USE_INTERFACE:\n","    demo = gr.Interface(\n","    fn=lambda query: answer_query_with_context(query, df_excerpt, doc_embeddings),\n","    inputs=gr.Textbox(lines=2,  label=\"Query\", placeholder=\"Type Question Here...\"),\n","    outputs=gr.Textbox(lines=2, label=\"Answer\"),\n","    description=\"Example of a domain-specific chatbot, using ChatGPT with supplemental content and fine-tuning.<br>\\\n","                  Here, the content relates to the investment outlook for 2023, according to Morgan Stanley, JPMorgan and Goldman Sachs.<br>\\\n","                  Sample queries: What is Goldman's outlook for inflation? What about the bond market? What does JPMorgan think about 2023?<br>\\\n","                  NOTE: High-level demo only. Supplemental content used here limited to about 30 paragraphs, due to limits on free-of-charge usage of ChatGPT.<br>\\\n","                  Far more robust domain-specific responses are possible.\",\n","    title=\"Fine-Tuned Domain-Specific Chatbot\",)\n","    # Launch the interface\n","    demo.launch(debug=True) # To show errors in colab notebook, set debug=True in launch()\n","else:\n","    prompt = construct_prompt(\n","        'What is the outlook for inflation?',\n","        document_embeddings,\n","        df_excerpt\n","    )\n","    # print(\"===\\n\", prompt) # FOR TESTING ONLY\n","    answer_query_with_context(\"What is Goldman's outlook for inflation?\", df_excerpt, document_embeddings)  "],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"4GLXbaNvkBUC","outputId":"d6da8621-cf13-4186-dafd-7a021a13f51b","executionInfo":{"status":"ok","timestamp":1675172402235,"user_tz":300,"elapsed":81233,"user":{"displayName":"Bob McKee","userId":"09961974450561244966"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n","Note: opening Chrome Inspector may crash demo inside Colab notebooks.\n","\n","To create a public link, set `share=True` in `launch()`.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["(async (port, path, width, height, cache, element) => {\n","                        if (!google.colab.kernel.accessAllowed && !cache) {\n","                            return;\n","                        }\n","                        element.appendChild(document.createTextNode(''));\n","                        const url = await google.colab.kernel.proxyPort(port, {cache});\n","\n","                        const external_link = document.createElement('div');\n","                        external_link.innerHTML = `\n","                            <div style=\"font-family: monospace; margin-bottom: 0.5rem\">\n","                                Running on <a href=${new URL(path, url).toString()} target=\"_blank\">\n","                                    https://localhost:${port}${path}\n","                                </a>\n","                            </div>\n","                        `;\n","                        element.appendChild(external_link);\n","\n","                        const iframe = document.createElement('iframe');\n","                        iframe.src = new URL(path, url).toString();\n","                        iframe.height = height;\n","                        iframe.allow = \"autoplay; camera; microphone; clipboard-read; clipboard-write;\"\n","                        iframe.width = width;\n","                        iframe.style.border = 0;\n","                        element.appendChild(iframe);\n","                    })(7860, \"/\", \"100%\", 500, false, window.element)"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["GETTING DOC SIMILARIITIES.........\n","FINISHED DOC SIMILARITIES..............\n","Selected 4 document sections:\n","20\n","19\n","2\n","9\n","Given the following context, answer the question as truthfully as possible, and if the answer is not contained within the context below, say \"Sorry, I don't know.\"\n","\n","Context:\n","\n","* JPMorgan says:  Commodity price forecasts 2023. Commodity price forecasts for 2023, with Brent averaging $90 per barrel, WTI averaging $83 and gold averaging $1,860 in the fourth quarter of 2023. There are strong reasons to expect a relatively robust 1.3 million barrels per day (mbd) of oil demand growth next year, despite expectations for the global economy to expand at a sub-par 1.5% pace in 2023. There is still substantial room for a cyclical rebound, driven by a continued normalization of demand for mobility fuels like gasoline, diesel and jet fuel to pre-COVID levels.“Our forecast of a $90 Brent in 2023 centers on the view that the OPEC+ alliance (Organization of the Petroleum Exporting Countries and allies) will do the heavy lifting to keep markets balanced next year,” added Kaneva. On the structural side, expansion of the world’s oil supply growth is expected to slow in 2024, reviving the need for OPEC’s crude. Growth from U.S. shale producers, traditionally the most responsive to changing market conditions, is expected to more than halve from 1.1-1.2 mbd this year and next to 0.5 mbd in 2024.\n","* JPMorgan says:  Commodities Outlook. Entering 2022, the view was the global oil market would remain tight but balanced, with Brent averaging $90 per barrel (bbl) for the year. With the , J.P. Morgan Research opted to raise its 2022 average Brent price to $104/bbl and 2023 price to $98/bbl, with prices peaking in the second quarter of 2022 at $114/bbl.  “After maintaining our price view for eight months, we now opt to shave $8 off our 2023 price projections, on our expectations that Russian production will fully normalize to pre-war levels by mid-2023. Despite more pessimistic expectations for balances over the next few months, we find the underlying trends in the oil market supportive and expect global Brent benchmark price to average $90/bbl in 2023 and $98/bbl in 2024,” said Natasha Kaneva, Head of Global Commodities Strategy at J.P. Morgan.  \n","* Morgan Stanley says: Other key takeaways from our 2023 Strategy Outlook: 10-year Treasury yields will end 2023 at 3.5% vs. a 14-year high of 4.22% in October 2022. With favorable pricing, securitized products, such as mortgage-backed securities, will offer upside. S&P 500 will tread water, ending 2023 around 3,900, but with material swings along the way. U.S. dollar will peak in 2022 and declines through 2023. Emerging-market and Japanese equities could deliver double-digit returns. Oil will outperform gold and copper, with Brent crude, the global oil benchmark, ending 2023 at $110.\n","* JPMorgan says: 2023 Market Outlook: Stocks Set to Fall Near-Term as Economic Growth Slows . The global economy is projected to expand at a sluggish pace of around 1.6% in 2023 as financial conditions tighten, the winter aggravates China’s COVID policy and Europe’s natural gas problems persist.  The global economy is not at imminent risk of sliding into recession, as the sharp decline in inflation helps promote growth, but the J.P. Morgan Research baseline view assumes a U.S. recession is likely before the end of 2023.  In the first half of 2023, the S&P 500 is expected to re-test the lows of 2022, but a pivot from the Fed could drive an asset recovery later in the year, pushing the S&P 500 to 4,200 by year-end.\n","\n","Question: what is the outlook for oil?\n","\n","###\n","\n","\n","Keyboard interruption in main thread... closing server.\n"]}]}],"metadata":{"kernelspec":{"display_name":"Python 3.9.9 64-bit ('3.9.9')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.9"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"cb9817b186a29e4e9713184d901f26c1ee05ad25243d878baff7f31bb1fef480"}},"colab":{"provenance":[{"file_id":"1W_Bej3Y1LaXKnR_iGFEx6Da2wuZm12Q9","timestamp":1675172438106},{"file_id":"1mu7iR1tety9b98JTVyEg7CJCP1TH8fF5","timestamp":1675169284638},{"file_id":"1FvdcuhqmA6gmnSDfN8-YGq2HNFbDlrJW","timestamp":1674744969893},{"file_id":"1BbEl7z86doyTfIlbJ0ftPUneIdWfLmPR","timestamp":1674741907520},{"file_id":"1Su__ejB7IhbR5F-n6LbTTL6Rvp7vi40M","timestamp":1674584399652},{"file_id":"1LnXbpPQ6bOzU4tBoSkswm1jWF9_W3wg2","timestamp":1674568128334},{"file_id":"https://github.com/robjm16/openai-cookbook/blob/main/examples/fine-tuned_qa/olympics-1-collect-data.ipynb","timestamp":1674255334260}]}},"nbformat":4,"nbformat_minor":0}
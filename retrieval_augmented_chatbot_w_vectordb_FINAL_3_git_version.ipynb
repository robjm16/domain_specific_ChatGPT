{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieval-Based Chatbot with Vector Database Integration"
      ],
      "metadata": {
        "id": "jgHTso_U9goT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This program demonstrates how OpenAI's ChatGPT language model can be used to answer questions in specific domain areas, using a process called retrieval-based augmentation and supported by a vector database. \n",
        "\n",
        "With such an approach, companies can leverage ChatGPT's extraordinary natural language capabilities while limiting its answers to company-specific documents and information. The vector database enables the process by efficiently storing, managing and querying word vectors (or \"embeddings\") associated with a company's knowledge base.  These vectors play a critical role in large language models such as ChatGPT.  \n",
        "\n",
        "The program includes integration with OpenAI and Pinecone, a cloud-based vector database provider, via their APIs.  \n",
        "\n",
        "The program asks a user for a question in a prescribed domain area.  It then compares the user's query against pre-loaded domain content to identify and retrieve the most useful sections of content. The program answers the user question by leveraging ChatGPT's powerful general capabilities with the newly incorporated domain knowledge.  Such an approach might be used, \n",
        "for example, to provide an insurance company's agents with the ability to answer customer questions based on the company's policy materials.\n",
        "\n",
        "For this example, I used the 2023 investment outlook summaries of leading Wall Street banks as the domain-specific content.   The summaries were drawn from the websites of \n",
        "Morgan Stanley ([here](https://www.morganstanley.com/ideas/global-investment-strategy-outlook-2023)), \n",
        "JPMorgan ([here](https://www.jpmorgan.com/insights/research/market-outlook)) and \n",
        "Goldman Sachs ([here](https://www.goldmansachs.com/insights/pages/gs-research/macro-outlook-2023-this-cycle-is-different/report.pdf)).  Questions such as \"What is the outlook for inflation?\" and \"What will happen to the price of oil?\" can be posed and answered via the chatbot. \n",
        "\n",
        "See the article [\"Scaling Company Chatbots with Vector Databases\"](retrieval_based_chatbot_article.md) for a more in-depth discussion of this program and vector databases. \n"
      ],
      "metadata": {
        "id": "byseF9UndPRA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Libraries and Imports"
      ],
      "metadata": {
        "id": "pahdYigtC1ZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install openai \n",
        "! pip install transformers \n",
        "! pip install gradio \n",
        "! pip install python-docx \n",
        "! pip install pandas \n",
        "! pip install pinecone-client "
      ],
      "metadata": {
        "id": "diGKBV0flitc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import docx\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import openai\n",
        "import gradio as gr\n",
        "import pickle\n",
        "import os\n",
        "from transformers import GPT2TokenizerFast\n",
        "import pinecone\n",
        "import time"
      ],
      "metadata": {
        "id": "weIrzIuvGEyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables"
      ],
      "metadata": {
        "id": "zc0mkl2nCk8o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "openai.api_key = \"YOUR_OPENAI_API_KEY\"   # Your API keys go here \n",
        "pinecone_key= \"YOUR_PINECONE_API_KEY\"  \n",
        "DOC_FILEPATH = 'Compilation_investment_outlook_2023.docx' # Path to document containing domain content; update as needed  \n",
        "COMPLETIONS_MODEL = \"text-davinci-003\"  \n",
        "DOC_EMBEDDINGS_MODEL = \"text-embedding-ada-002\"\n",
        "QUERY_EMBEDDINGS_MODEL = \"text-embedding-ada-002\"\n",
        "MAX_SECTION_LEN =1100  # The API limits total tokens -- for the prompt containing the question and domain-specific content and the answer -- to 2048 tokens, or about 1500 words.  \n",
        "SEPARATOR = \"\\n* \"  # A string called SEPARATOR is defined as the newline character followed by an asterisk and a space. This string will be used as a separator between different pieces of text.\n",
        "TOKENIZER = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
        "SEPARATOR_LEN = len(TOKENIZER.tokenize(SEPARATOR))\n",
        "COMPLETIONS_API_PARAMS = {\n",
        "    # We use temperature of 0.0 because it gives the most predictable, factual answer.\n",
        "    \"temperature\": 0.0,\n",
        "    \"max_tokens\": 300,\n",
        "    \"model\": COMPLETIONS_MODEL,\n",
        "}\n",
        "EMBEDDING_DIMENSION = 1536  # The dimensionality of the document embeddings"
      ],
      "metadata": {
        "id": "sTvPtcUae5sS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Functions: Initial Text Processing "
      ],
      "metadata": {
        "id": "xxa--DKAC67O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# FUNCTIONS: INITIAL TEXT PROCESSING \n",
        "\n",
        "def load_text(DOC_FILEPATH):\n",
        "  \"\"\"\n",
        "  Loads a Microsoft Word document and returns a DataFrame containing the text of each paragraph in the document.\n",
        "\n",
        "  Input:\n",
        "    DOC_FILEPATH (str): the filepath to the Microsoft Word document.\n",
        "    \n",
        "  Returns:\n",
        "    df (pandas.DataFrame): a DataFrame containing the 'content' column with the text of each paragraph in the document.\n",
        "  \"\"\"\n",
        "  # Open the Word document\n",
        "  doc = docx.Document(DOC_FILEPATH)\n",
        "\n",
        "  # Create an empty pandas DataFrame\n",
        "  df = pd.DataFrame()\n",
        "\n",
        "  # Iterate through the paragraphs in the document and add each to the df\n",
        "  for i, p in enumerate(doc.paragraphs):\n",
        "\n",
        "      # Add the paragraph text [and index to the DataFrame]    \n",
        "      df.loc[i, 'content'] = p.text\n",
        "      # df.loc[i, 'paragraph_index'] = i\n",
        "\n",
        "  # Delete empty paragraphs\n",
        "  df['content'] = df['content'].replace('', np.nan)\n",
        "  df = df.dropna(axis=0, subset=['content']).reset_index(drop=True)\n",
        "\n",
        "  return df\n",
        "    \n",
        "def count_tokens(row):\n",
        "    \"\"\"count the number of tokens in a string\"\"\"\n",
        "    return len(TOKENIZER.encode(row))\n",
        "\n",
        "def truncate_text(df):\n",
        "    \"\"\"\n",
        "    Truncates the text in the 'content' column of the input DataFrame if the number of tokens \n",
        "    in the text exceeds a specified maximum number. It will set the truncated text and the \n",
        "    number of tokens in the 'content' and 'tokens' columns, respectively.\n",
        "\n",
        "    Input:\n",
        "    df (pandas.DataFrame): a DataFrame containing the 'content' column\n",
        "\n",
        "    Returns:\n",
        "    df (pandas.DataFrame): the input DataFrame with modified 'content' and 'tokens' columns.\n",
        "\n",
        "    \"\"\"\n",
        "    for i in range(len(df)):\n",
        "        if df['tokens'][i] > 590:\n",
        "            text = df['content'][i]\n",
        "            tokens = TOKENIZER.encode(text)\n",
        "            truncated_tokens = tokens[:590]\n",
        "            truncated_text = TOKENIZER.decode(truncated_tokens)\n",
        "            df.at[i, 'content'] = truncated_text\n",
        "            df.at[i, 'tokens'] = len(truncated_tokens)\n",
        "    return df\n",
        "\n"
      ],
      "metadata": {
        "id": "ybpU7xUrV4Sk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Functions: Embeddings and Pinecone Integration"
      ],
      "metadata": {
        "id": "FPnQILaGDCJc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# FUNCTIONS: EMBEDDING/PINECONE VECTOR DB\n",
        "\n",
        "def compute_doc_embeddings(df):\n",
        "    \"\"\"\n",
        "    Generates embeddings for each row in a Pandas DataFrame using the OpenAI document embeddings model,\n",
        "    and uploads the embeddings to Pinecone.\n",
        "\n",
        "    Args:\n",
        "        df (pandas.DataFrame): The DataFrame for which to generate embeddings.\n",
        "    \n",
        "    Returns:\n",
        "        dict: A dictionary that maps the indices of the rows to their corresponding embedding vectors.\n",
        "    \"\"\"\n",
        "    embeddings = {}\n",
        "    for idx, r in df.iterrows():\n",
        "        embedding = get_doc_embedding(r.content.replace(\"\\n\", \" \"))\n",
        "        embeddings[str(idx)] = embedding\n",
        "    # items = [(key, embeddings[key][:3]) for key in list(embeddings.keys())[:3]] # For Testing Purposes ........\n",
        "    # for key, values in items:\n",
        "    #     print(key, values)    \n",
        "    upload_embeddings_to_pinecone(embeddings)\n",
        "    return embeddings\n",
        "\n",
        "def get_doc_embedding(text):\n",
        "    \"\"\"\n",
        "    Generates an embedding for the given text using the OpenAI document embeddings model.\n",
        "\n",
        "    Args:\n",
        "        text (str): The text to generate an embedding for.\n",
        "    \n",
        "    Returns:\n",
        "        numpy.ndarray: The embedding vector for the given text.\n",
        "    \"\"\"\n",
        "    # print(text) # For debugging\n",
        "\n",
        "    # Call the OpenAI API to generate the embedding\n",
        "    result = openai.Embedding.create(\n",
        "        model=DOC_EMBEDDINGS_MODEL,\n",
        "        input=[text]\n",
        "    )\n",
        "\n",
        "    # Extract the embedding vector from the API response\n",
        "    # print(result[\"data\"][0][\"embedding\"][:3])  # For debugging\n",
        "\n",
        "    return result[\"data\"][0][\"embedding\"]\n",
        "\n",
        "\n",
        "def upload_embeddings_to_pinecone(embeddings):\n",
        "    \"\"\"\n",
        "    Uploads the provided embeddings to Pinecone.\n",
        "    \n",
        "    Args:\n",
        "        embeddings (dict): A dictionary mapping document indices to their embeddings.\n",
        "    \"\"\"\n",
        "    # Transform the dictionary to a list of tuples\n",
        "    transformed_list = [(str(key), value) for key, value in embeddings.items()]\n",
        "    pinecone_client.upsert(transformed_list)\n",
        "\n",
        "\n",
        "def fetch_embeddings_from_pinecone(df, pinecone_client):\n",
        "    \"\"\"\n",
        "    Fetches all embeddings from the specified Pinecone index.\n",
        "    \n",
        "    Args:\n",
        "        pinecone_index (str): The name of the Pinecone index from which to fetch.\n",
        "    \n",
        "    Returns:\n",
        "        dict: A dictionary mapping document indices to embeddings.\n",
        "    \"\"\"\n",
        "    # Get all item ids in the index\n",
        "    item_ids = [str(i) for i in df.index]\n",
        "    # Fetch the vectors for all items\n",
        "    document_embeddings = pinecone_client.fetch(ids=item_ids)\n",
        "    \n",
        "    return document_embeddings\n",
        "    "
      ],
      "metadata": {
        "id": "A7Zwad6hjhaE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Functions: Question Answering"
      ],
      "metadata": {
        "id": "I-lNT8FjDMa_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# FUNCTIONS: QUESTION ANSWERING \n",
        "\n",
        "def get_embedding(text, model): \n",
        "    \"\"\"\n",
        "    Generates an embedding for the given text using the specified OpenAI model.\n",
        "    \n",
        "    Args:\n",
        "        text (str): The text for which to generate an embedding.\n",
        "        model (str): The name of the OpenAI model to use for generating the embedding.\n",
        "    \n",
        "    Returns:\n",
        "        numpy.ndarray: The embedding for the given text.\n",
        "    \"\"\"\n",
        "    result = openai.Embedding.create(\n",
        "      model=model,\n",
        "      input=[text]\n",
        "    )\n",
        "    return result[\"data\"][0][\"embedding\"]\n",
        "\n",
        "\n",
        "def get_query_embedding(text):\n",
        "   \"\"\"\n",
        "    Generates an embedding for the given text using the OpenAI query embeddings model.\n",
        "    \n",
        "    Args:\n",
        "        text (str): The text for which to generate an embedding.\n",
        "    \n",
        "    Returns:\n",
        "        numpy.ndarray: The embedding for the given text.\n",
        "    \"\"\"\n",
        "   return get_embedding(text, QUERY_EMBEDDINGS_MODEL)\n",
        "\n",
        "\n",
        "def answer_query_with_context(query, df, document_embeddings, show_prompt: bool = False):\n",
        "    # print(\"STARTING ANSWER QUERY WITH CONTEXT..............................\") # -- FOR TESTING PURPOSES\n",
        "    prompt = construct_prompt(query, df)\n",
        "    \"\"\"\n",
        "    Answer a query using relevant context from a DataFrame.\n",
        "    \n",
        "    Args:\n",
        "        query (str): The query to answer.\n",
        "        df (pandas.DataFrame): A DataFrame containing the document sections.\n",
        "        document_embeddings (dict): A dictionary mapping document embeddings to their indices.\n",
        "        show_prompt (bool, optional): If `True`, print the prompt before generating a response.\n",
        "    \n",
        "    Returns:\n",
        "        str: The generated response to the query.\n",
        "    \"\"\"   \n",
        "    if show_prompt:\n",
        "        print(prompt)\n",
        "\n",
        "    response = openai.Completion.create(\n",
        "                prompt=prompt,\n",
        "                **COMPLETIONS_API_PARAMS\n",
        "                )\n",
        "\n",
        "    return response[\"choices\"][0][\"text\"].strip(\" \\n\")\n",
        "\n",
        "def construct_prompt(question, df):\n",
        "    \"\"\"\n",
        "    Construct a prompt for answering a question using the most relevant document sections.\n",
        "    \n",
        "    Args:\n",
        "      question (str): The question to answer.\n",
        "      # context_embeddings (dict): A dictionary mapping document embeddings to their indices.\n",
        "      df (pandas.DataFrame): A DataFrame containing the document sections.\n",
        "    \n",
        "    Returns:\n",
        "      str: The prompt, including the question and the relevant context.\n",
        "    \"\"\"\n",
        "  \n",
        "    # print(\"STARTING CONSTRUCT PROMPT..............................\") # -- FOR TESTING PURPOSES\n",
        "    # Get the query embedding from the OpenAI api\n",
        "    xq = openai.Embedding.create(input=question, engine=QUERY_EMBEDDINGS_MODEL)['data'][0]['embedding']\n",
        "\n",
        "    # Get the top n document sections related to the query from the pinecone database\n",
        "    res = pinecone_client.query([xq], top_k=5, include_metadata=True)\n",
        "\n",
        "    # Extract the section indexes for the top n sections\n",
        "    most_relevant_document_sections = [int(match['id']) for match in res['matches']]\n",
        "\n",
        "    # print(f\"TESTING... Most relevant document sections: {most_relevant_document_sections}\") # -- FOR TESTING PURPOSES\n",
        "\n",
        "    ## LEAVE AS IS #######################################\n",
        "    chosen_sections = []\n",
        "    chosen_sections_len = 0\n",
        "    chosen_sections_indexes = []\n",
        "     \n",
        "    for section_index in most_relevant_document_sections:\n",
        "        # Add contexts until we run out of space.        \n",
        "        document_section = df.loc[section_index]\n",
        "        \n",
        "        chosen_sections_len += document_section.tokens + SEPARATOR_LEN\n",
        "        if chosen_sections_len > MAX_SECTION_LEN:\n",
        "            break\n",
        "            \n",
        "        chosen_sections.append(SEPARATOR + document_section.content.replace(\"\\n\", \" \"))\n",
        "        chosen_sections_indexes.append(str(section_index))\n",
        "            \n",
        "  \n",
        "    # print(f\"Selected {len(chosen_sections)} document sections:\") # -- FOR TESTING PURPOSES\n",
        "    # print(\"\\n\".join(chosen_sections_indexes)) # -- FOR TESTING PURPOSES\n",
        "    \n",
        "    header = \"\"\"Answer the question as truthfully as possible using the provided context, and if the answer is not contained within the text below, say \"Sorry, I don't know.\"\\n\\nContext:\\n\"\"\"\n",
        "\n",
        "    full_prompt = header + \"\".join(chosen_sections) + \"\\n\\n Q: \" + question + \"\\n A:\"\n",
        "\n",
        "    # print(full_prompt) # FOR TESTING PURPOSES\n",
        "\n",
        "    return full_prompt\n"
      ],
      "metadata": {
        "id": "4LCUc8Aaj0W8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Main Program"
      ],
      "metadata": {
        "id": "cd2rIf2-DSrb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Initial Text Processing"
      ],
      "metadata": {
        "id": "ux-UZmGeDW8q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the text into dataframe \n",
        "df = load_text(DOC_FILEPATH)\n",
        "\n",
        "# Count the tokens \n",
        "df = df.copy()    \n",
        "df['tokens'] = df['content'].apply(count_tokens)\n",
        "\n",
        "# Call the truncate_text function on the dataframe to fall within word/token limits \n",
        "df = df.copy()    \n",
        "df = truncate_text(df)\n",
        "\n",
        "# print(df.head(10))   # FOR TESTING PURPOSES \n",
        "# print(df['content'][3])   # FOR TESTING PURPOSES"
      ],
      "metadata": {
        "id": "fprcNwvzDvNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Document Embeddings and Pinecone Integration"
      ],
      "metadata": {
        "id": "BbO3DErRDqGQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Pinecone\n",
        "pinecone.init(api_key=pinecone_key, environment=\"asia-southeast1-gcp-free\")\n",
        "\n",
        "# Create Pinecone index -- use only if first time setting up Pinecone index for a specific project  \n",
        "# pinecone.create_index(name=\"docembeddings\", dimension=1536, metric=\"cosine\", shards=1)\n",
        "\n",
        "# Connect to Pinecone service\n",
        "pinecone_client = pinecone.Index(index_name=\"docembeddings\")\n",
        "\n",
        "# Use code below if calculating the embeddings in real time via OpenAI API...\n",
        "# document_embeddings = compute_doc_embeddings(df) \n",
        "\n",
        "# OR ... use code below if importing previously-loaded embeddings from Pinecone\n",
        "document_embeddings = fetch_embeddings_from_pinecone(df, pinecone_client)\n",
        "\n",
        "# # FOR TESTING PURPOSES - PRINT FIRST FIVE VECTOR VALUES\n",
        "# my_vectors = document_embeddings # For Testing Only .................\n",
        "# test = [item['values'] for item in my_vectors['vectors'].values()] # For Testing Only .................\n",
        "# print(f\"TESTING...First 5 vector values are: {test[0][:5]}\") # For Testing Only ................."
      ],
      "metadata": {
        "id": "Odx5ZVgtD_KT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Question Answering and Interface"
      ],
      "metadata": {
        "id": "Fj8_zFUTDp1x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "use_interface = True # Change to False if you want to run the code without the Gradio interface, and instead see a single pre-supplied question \n",
        "\n",
        "if use_interface:\n",
        "    demo = gr.Interface(\n",
        "    fn=lambda query: answer_query_with_context(query, df, document_embeddings),\n",
        "    inputs=gr.Textbox(lines=2,  label=\"Query\", placeholder=\"Type Question Here...\"),\n",
        "    outputs=gr.Textbox(lines=2, label=\"Answer\"),\n",
        "    description=\"Example of a domain-specific chatbot, using ChatGPT with supplemental content added.<br>\\\n",
        "                  Here, the content relates to the investment outlook for 2023, according to Morgan Stanley, JPMorgan and Goldman Sachs.<br>\\\n",
        "                  Sample queries: What is Goldman's outlook for inflation? What about the bond market? What does JPMorgan think about 2023?<br>\\\n",
        "                  NOTE: High-level demo only. Supplemental content used here limited to about 30 paragraphs, due to limits on free-of-charge usage of ChatGPT.<br>\\\n",
        "                  \",\n",
        "    title=\"Domain-Specific Chatbot\",)\n",
        "    demo.launch(debug=True)  # Set debug=True in launch() to see testing/debugging output\n",
        "else:\n",
        "    sample_question = 'What is the outlook for inflation?'\n",
        "    print(answer_query_with_context(sample_question, df, document_embeddings)) "
      ],
      "metadata": {
        "id": "rBQgbDXPj9iA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 633
        },
        "outputId": "e7d9aca6-a386-4534-f700-b38fdf3cdef5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Note: opening Chrome Inspector may crash demo inside Colab notebooks.\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, width, height, cache, element) => {\n",
              "                        if (!google.colab.kernel.accessAllowed && !cache) {\n",
              "                            return;\n",
              "                        }\n",
              "                        element.appendChild(document.createTextNode(''));\n",
              "                        const url = await google.colab.kernel.proxyPort(port, {cache});\n",
              "\n",
              "                        const external_link = document.createElement('div');\n",
              "                        external_link.innerHTML = `\n",
              "                            <div style=\"font-family: monospace; margin-bottom: 0.5rem\">\n",
              "                                Running on <a href=${new URL(path, url).toString()} target=\"_blank\">\n",
              "                                    https://localhost:${port}${path}\n",
              "                                </a>\n",
              "                            </div>\n",
              "                        `;\n",
              "                        element.appendChild(external_link);\n",
              "\n",
              "                        const iframe = document.createElement('iframe');\n",
              "                        iframe.src = new URL(path, url).toString();\n",
              "                        iframe.height = height;\n",
              "                        iframe.allow = \"autoplay; camera; microphone; clipboard-read; clipboard-write;\"\n",
              "                        iframe.width = width;\n",
              "                        iframe.style.border = 0;\n",
              "                        element.appendChild(iframe);\n",
              "                    })(7860, \"/\", \"100%\", 500, false, window.element)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
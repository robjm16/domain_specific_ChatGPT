{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VmrTrK38Z-tv",
        "outputId": "4da6b467-f8d3-41d1-fcac-ec8db48d87aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Leveraging ChatGPT for Business and Organizational Purposes  \n",
        "\n",
        "Since its introduction in November 2022, the ChatGPT chatbot has captivated the public with its ability to answer questions on virtually any topic in human-like fashion.  Not to mention its ability to compose personalized poems  in a matter of seconds and to write computer code based on natural language instructions.  \n",
        "\n",
        "ChatGPT will no doubt have a huge impact on the public – as well as on businesses and other organizations.\n",
        "\n",
        "Here, the trick will be to leverage ChatGPT's awesome generalist powers across specific domain areas, such as an industry of function.\n",
        "\n",
        "An insurance company, for example, might tailor ChatGPT's answers to information in its policy documents. The answers could be used internally to train and inform service reps, as well as with customers directly via a website chatbox. \n",
        "\n",
        "Industries and functions that are knowledge- or service-intensive will benefit most from ChatGPT’s powers – for example, healthcare, education, professional services, IT, marketing and sales.   \n",
        "\n",
        "ChatGPT has the potential to remake how businesses and other institutions operate and interact with customers.\n",
        "\n",
        "## Building Domain-Specific Capabilities\n",
        "Developed by OpenAI, a research company partially owned by Microsoft, ChatGPT was \"trained\" on a massive trove of text data on the internet through 2021 -- some 300 billion words from web pages, books and other documents. \n",
        "\n",
        "Due to this date cut off, ChatGPT knows all about, say, John F. Kennedy, but it knows nothing about events that occurred in 2022 and later. Nor does it know anything about company and organization documents that were not available to it in its training. \n",
        "\n",
        "But through an API (Application Programming Interface, which lets different computer programs talk with each other), ChatGPT can process and incorporate new information in real-time. This feature enables it to stay up-to-date with the latest developments or specific knowledge in a particular industry or field.\n",
        "\n",
        "To demonstrate how this might work, I built a domain-specific chatbox (here)[]. In my example, I took the 2023 investment outlook summaries posted to the web  by Morgan Stanley [(here)]([https://colab.research.google.com/drive/1iuAQSuSJarzafsWfDOrU8LnPa7f89DPo#scrollTo=r53QnNidiGf9&line=22&uniqifier=1]), JPMorgan [(here)](https://www.jpmorgan.com/insights/research/market-outlook) and Goldman Sachs [(here)](https://www.goldmansachs.com/insights/pages/gs-research/macro-outlook-2023-this-cycle-is-different/report.pdf) and combined them into one 4,000 word document. (Note: Most of the my code was adapted from OpenAI's cookbook of code examples for leveraging the ChatGPT model.\n",
        "\n",
        "Through a process described in more detail below, that information was fed into ChatGPT and became the basis for responses to questions such as: \"What does Goldman see happening with inflation in 2023?\" and \"What is the outlook for the bond market?\"\n",
        "\n",
        "Below is an overview of what I found about ChatGPT,  written for both technical and general audiences. \n",
        "\n",
        "(By the way, GPT stands for “Generative Pre-trained Transformer,” a technical reference to the AI model.)\n",
        "\n",
        "## ChatGPT’s Many Uses \n",
        "ChatGPT’s capabilities go well beyond what a traditional chatbox offers: \n",
        "- It can be used to draft copy for marketing materials, blog posts and product descriptions. \n",
        "- It can edit, summarize or translate a document, and write in almost any voice (e.g., as a pirate).  \n",
        "- It can be used for text classification – for example, whether tweets about my organization are positive or negative this week.      \n",
        "- It can search documents via “semantic search,” which understands the intent of your search query, not just the exact words. \n",
        "\n",
        "On the computer coding side: \n",
        "- It can convert written instructions into computer code.\n",
        "- It can auto-complete code.\n",
        "- It can explain and document your code. \n",
        "- It can write test cases and fix bugs.\n",
        "- It can convert between coding languages (e.g., Java to Python)  \n",
        "   \n",
        "\n",
        "## Two Key Mechanisms: Prompt and Completion \n",
        "When interacting with ChatGPT, either through a simple web interface or through your computer code via an API, the prompt and completion mechansims are key.\n",
        "\n",
        "The prompt is an input mechansim where you place your question or request, as well as any context, including domain-specific content (e.g., the investment banks’ 2023 outlooks) and intructions (e.g., respond in a particular format).\n",
        "\n",
        "The completion is ChatGPT’s response to your prompt.  It answers your question or request.  Importantly, it contains a parameter called “temperature,” which controls how creative ChatGPT should be in responding to a prompt.  A lower temperature means ChatGPT should be conservative, sticking to the most factual information.  But there are times when you might be looking for more creativity and using a higher temperature makes sense. \n",
        "\n",
        "## Domain-Specific Uses: Several Technical Approaches\n",
        "1. Use as is:  The first approach is to simply use ChatGPT as is.  For example, ChatGPT has well-honed classification capabilities, and it may not benefit much from additional specific examples.  If you want to use ChatGPT to rate sentiments in hotel or restaurant reviews from the internet, its inherent capabilities should work fine. \n",
        "\n",
        "2. Inject content into prompts: The second approach, which I took in my example, is to inject domain-specific context into your prompt.  In this  scenario, ChatGPT uses its well-practiced natural language capabilities, but then looks to the specific content when formulating an answer.\n",
        "\n",
        "This approach and the next use a technique known as “in-context” learning.  Either way, it’s important to note that the GPT3 model is not “retrained” in the traditional sense.  Instead, it makes predictions based on the supplied context. \n",
        "\n",
        "3. Fine tune a model:  Currently, only the previous and less powerful version of ChatGPT’s neural network model (GPT2) is available for download and use in your own environment.  With GPT2 and many pre-trained libraries, you can go in and change fundamental aspects of the model, including its shape and size, and retrain it on your domain-specific content.  \n",
        "\n",
        "The newest model (GPT3) can only be accessed via the OpenAI API.  You can “fine tune” it on up to 100 specific pieces of content and save a proprietary version of it (at OpenAI) for future use via the API.  But you cannot fundamentally retrain it.   \n",
        "\n",
        "Instead, similar to the second approach above, you create a new version and then feed it up to 100 domain-specific pieces of content.  The model will then run in the background at OpenAI, seeking to maximize correct answers. Many of the model’s parameters (see discussion of neural networks below) will be updated, but that is done in the background.  When complete, it creates a new version, with a new name you give it. \n",
        "\n",
        "The key difference between the second and third approaches above is that approach two injects the domain specific content in real time into the prompt whereas approach three tailors the model to your needs and produces a reusable customized model, with potentially more accurate results.  With approach two, the base model is used unchanged and the model retains no \"memory\" of the injected content.  \n",
        "\n",
        "## Word Embeddings: 4,000 Shades of Meaning \n",
        "When ChatGPT receives a question, it maps each word or word fragment to a token, a unique numerical identifier.  With ChatGPT, each token represents approximately 0.75 words.  (The math is important due to free usage limits and potential extra fees from using ChatGPT.)\n",
        "\n",
        "Each token also has a numerical representation of the word or word fragment called an \"embedding.\" For example, the word \"queen\" can be represented by a series of numerical sequences capturing how close the word is semantically to words such as \"king,\" \"female” and “leader.\"  The embedding also captures syntax and context, such as a speaker’s intent.  \n",
        "\n",
        "In ChatGPT's case, eached each word has 4,096 data points or dimensions associated with it. In addition, ChatGPT's artifical intelligence model -- a deep neural network -- pays attention to words that come before and after, so it holds on to context as it \"reads in\" new words.  More on the neural network below.##  \n",
        "\n",
        "The embedding for the fourth token in a text might look like this: \n",
        "(4, [0.016102489084005356, -0.011134202592074871, …, 0.01891878806054592])\n",
        "\n",
        "## GPT3: One of World’s Largest Neural Networks \n",
        "Neural networks are often described as brain-like, with “nodes” and connections called “synapses.” In the simple example below, the first layer going left to right takes in input (such as the embeddings of a question) and the far right layer is the output (the answer or response).  In between, the input goes through many layers and nodes, depending on the complexity of the model.  This part is “hidden” in that what each node represents is not easily discernable.  \n",
        "\n",
        "The lines between the nodes (the synapses in the brain), receive a mathematical weighting that maximizes the chances that the output or response is correct.  These weightings are called parameters.   \n",
        "\n",
        "![image](https://github.com/robjm16/domain_specific_ChatGPT/blob/main/basic_nn.png)\n",
        "  \n",
        "\n",
        "  \n",
        "The ChatGPT model has 175 billion potential line weightings or parameters, but not all of them “fire” depending on the prompt.  By contrast, GPT2 has 1.5  billion parameters. For further reference, the human brain is believed to have up to 100 trillion synsapses. \n",
        "\n",
        "The ChatGPT’ model also has an “attention” mechanism that allows it to differentially weigh the importance of different parts of the input text, leading to a more coherent and fluent response.  \n",
        "\n",
        "In addition, the ChatGPT model was trained on how actual human beings rated  answers, helping to make responses not just correct but more human friendly.  \n",
        "\n",
        "## ChatGPT in Action:  My Bank Example \n",
        "The first step in leverage ChatGPT on domain-specific content is to gather the content and pre-process as needed.\n",
        "\n",
        "The ChatGPT API has limits on the amount of work it will do for free. Accordingly, I limited my example to about 4,000 words containing the three banks' investment outlooks. I further arranged the content into about 30 paragraphs.\n",
        "\n",
        "There is a limit of 2,048 tokens – or about 1,500 words – for both the prompt and completion (you can pay for higher limits).  While my document is 4,00 words, only the most relevant sections are fed into the prompt, thus keeping below the token limit.  \n",
        "\n",
        "The document’s 30 paragraphs are first sent out to the ChatGPT API to get word embeddings. When a question is asked, that question also gets its respective embeddings via the API.\n",
        "\n",
        "Next, computer code on my machine compares the question to the content in the 30 paragraphs. It then picks the best-fit paragraphs based on how close the question is semantically to each paragraph (by doing a lot of math around their respective embeddings).\n",
        "\n",
        "The best paragaphs are then attached to the question as \"context\" and fed back to ChatGPT for an answer. My program also instructs ChatGPT to say, \"Sorry, I don't know,\" if it is asked a question where it does not have good information. (ChatGPT can be overly confident and completely wrong,  but there are ways to control for this.)\n",
        "\n",
        "Lastly, ChatGPt combines the question, the added domain content and the model's inherent natural language skills to produce a response.\n",
        "\n",
        "Below is an example of a question within the interface:\n",
        "\n",
        "![image](https://github.com/robjm16/domain_specific_ChatGPT/blob/main/interface_example.png)\n",
        " \n",
        "## The ChatGPT Ecosystem \n",
        "OpenAI was founded in 2015 by a group including Elon Musk, and, as mentioned earlier, Microsoft as a key partner and investor.  \n",
        "\n",
        "Microsoft plans to integrate ChatGPT with many of its offerings.  For example, it could be incorporated into Microsoft Word and PowerPoint apps for writing, summarization and editing purposes.  It could be used to augment Microsoft’s Bing search engine, providing direct answers to questions along with site links based on a more semantic search engine. ChatGPT’s coding assistance abilities could be integrated with Microsoft’s Visual Studio code editing product.  Microsoft already has Github Copilot, a code auto-completion tool, and some coders are already using Copilot and GPT3 in tandem to improve their productivity.  Lastly,  Micorosoft Azure’s cloud computing services could leverage GPT3 into its AI capabilities – for example, for large companies seeking to retrain ChatGPT on domain-specific content. \n",
        "\n",
        "The other large cloud providers – Google and Amazon Web Services (AWS) – will no doubt integrate GPT3 into their AI offerings.  But they presumably do not have the inside track that Microsoft seems to enjoy due to its partnership with OpenAI.  Google’s CEO has  reportedly called a “code red” following the release of ChatGPT, challenging the company to quickly incorporate Google’s own ChatGPT-like models into its dominant search platform. \n",
        "\n",
        "Google, in fact, developed several of the most powerful “large language models” similar to GPT3 (which go by the names BERT, T5 and XLNet).  Other leading large language models are Facebook’s RoBERTa and Salesforce’s CTRL. \n",
        "AWS’s suite of AI services is called SageMaker.  It includes pre-built algorithms and enables companies to quickly build, train and deploy machine learning models.\n",
        "\n",
        "Another player is Hugging Face, where my demo model is hosted.  Hugging Face hosts a popular community website for sharing open-source models and for prototyping and deploying natural language processing models. The platform includes a variety of tools and services for working with natural language models, including a library of pre-trained models, a framework for training and fine-tuning models, and an API for deploying models to production.  You can access and adapt GPT2 through Hugging Face (again, GPT3 is only available through the OpenAI API.) \n",
        "\n",
        "## Data Security\n",
        "Each organization will have to make its own security judgments around using ChatGPT,  including hosting and encryption issues.  ChatGPT says that information provided in prompts and saved customized versions of GPT3 developed via its API would never “leak” into ChatGPT’s wider training and thus somehow be exposed in a subsequent version of GPT.  However, companies will need to properly sanitize any documents used in prompts or training, and determine how much of the work should be performed behind their local or cloud provider’s firewalls.  \n",
        "\n",
        "There are related issues, including changing the model’s “temperature” settings to rein in ChatGPT’s  overconfidence, depending on the nature of the information and risks involved.     \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "r53QnNidiGf9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-LSJjvvEMuEW"
      },
      "outputs": [],
      "source": [
        "# +++++++++++  MASTER ++++++++++++++++"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "z_4qlFbMVNe7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4a00f17-d62b-4222-c15f-c0528c759468"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting openai\n",
            "  Downloading openai-0.26.1.tar.gz (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.3/55.3 KB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.8/dist-packages (from openai) (2.25.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from openai) (4.64.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from openai) (3.8.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->openai) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->openai) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->openai) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->openai) (2.10)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai) (2.1.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai) (4.0.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai) (22.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai) (1.8.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Building wheels for collected packages: openai\n",
            "  Building wheel for openai (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai: filename=openai-0.26.1-py3-none-any.whl size=67316 sha256=bc9c4cb68eaa0f8c692f8d66018653960ae7e12bb591981d87fc5af1b98577ee\n",
            "  Stored in directory: /root/.cache/pip/wheels/2f/9c/55/95d3609ccfc463eeffb96d50c756f1f1899453b85e92021a0a\n",
            "Successfully built openai\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.26.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m85.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.4/182.4 KB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.11.1 tokenizers-0.13.2 transformers-4.25.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gradio\n",
            "  Downloading gradio-3.16.2-py3-none-any.whl (14.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.2/14.2 MB\u001b[0m \u001b[31m72.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx\n",
            "  Downloading httpx-0.23.3-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 KB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pycryptodome\n",
            "  Downloading pycryptodome-3.16.0-cp35-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m86.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from gradio) (2.25.1)\n",
            "Requirement already satisfied: altair>=4.2.0 in /usr/local/lib/python3.8/dist-packages (from gradio) (4.2.0)\n",
            "Collecting markdown-it-py[linkify,plugins]\n",
            "  Downloading markdown_it_py-2.1.0-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 KB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.8/dist-packages (from gradio) (2022.11.0)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.8/dist-packages (from gradio) (1.10.4)\n",
            "Collecting ffmpy\n",
            "  Downloading ffmpy-0.3.0.tar.gz (4.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting fastapi\n",
            "  Downloading fastapi-0.89.1-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 KB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from gradio) (3.2.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from gradio) (2.11.3)\n",
            "Collecting uvicorn\n",
            "  Downloading uvicorn-0.20.0-py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 KB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from gradio) (3.8.3)\n",
            "Collecting aiofiles\n",
            "  Downloading aiofiles-22.1.0-py3-none-any.whl (14 kB)\n",
            "Collecting python-multipart\n",
            "  Downloading python-multipart-0.0.5.tar.gz (32 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting orjson\n",
            "  Downloading orjson-3.8.5-cp38-cp38-manylinux_2_28_x86_64.whl (140 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 KB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from gradio) (6.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from gradio) (1.3.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from gradio) (4.4.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.8/dist-packages (from gradio) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from gradio) (1.21.6)\n",
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Collecting websockets>=10.0\n",
            "  Downloading websockets-10.4-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.0/107.0 KB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: markupsafe in /usr/local/lib/python3.8/dist-packages (from gradio) (2.0.1)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.8/dist-packages (from altair>=4.2.0->gradio) (0.12.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.8/dist-packages (from altair>=4.2.0->gradio) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.8/dist-packages (from altair>=4.2.0->gradio) (4.3.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->gradio) (2022.7)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->gradio) (2.1.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->gradio) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->gradio) (1.8.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->gradio) (4.0.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->gradio) (22.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->gradio) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->gradio) (1.3.1)\n",
            "Collecting starlette==0.22.0\n",
            "  Downloading starlette-0.22.0-py3-none-any.whl (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.3/64.3 KB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting anyio<5,>=3.4.0\n",
            "  Downloading anyio-3.6.2-py3-none-any.whl (80 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.6/80.6 KB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sniffio\n",
            "  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
            "Collecting httpcore<0.17.0,>=0.15.0\n",
            "  Downloading httpcore-0.16.3-py3-none-any.whl (69 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.6/69.6 KB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.8/dist-packages (from httpx->gradio) (2022.12.7)\n",
            "Collecting rfc3986[idna2008]<2,>=1.3\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Collecting mdurl~=0.1\n",
            "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
            "Collecting mdit-py-plugins\n",
            "  Downloading mdit_py_plugins-0.3.3-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 KB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting linkify-it-py~=1.0\n",
            "  Downloading linkify_it_py-1.0.3-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->gradio) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->gradio) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->gradio) (0.11.0)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from python-multipart->gradio) (1.15.0)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->gradio) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->gradio) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->gradio) (2.10)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.8/dist-packages (from uvicorn->gradio) (7.1.2)\n",
            "Collecting h11>=0.8\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 KB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=3.0->altair>=4.2.0->gradio) (5.10.2)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=3.0->altair>=4.2.0->gradio) (0.19.3)\n",
            "Collecting uc-micro-py\n",
            "  Downloading uc_micro_py-1.0.1-py3-none-any.whl (6.2 kB)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from importlib-resources>=1.4.0->jsonschema>=3.0->altair>=4.2.0->gradio) (3.11.0)\n",
            "Building wheels for collected packages: ffmpy, python-multipart\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.0-py3-none-any.whl size=4711 sha256=aa71b9768780c8a6cddd942c163b04fb7336735077c018132c4ce4c82c0966c6\n",
            "  Stored in directory: /root/.cache/pip/wheels/ff/5b/59/913b443e7369dc04b61f607a746b6f7d83fb65e2e19fcc958d\n",
            "  Building wheel for python-multipart (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-multipart: filename=python_multipart-0.0.5-py3-none-any.whl size=31678 sha256=fe1dcdba82879843e3eb98c1cd73fbd985daaae7e983c7fbb0305ce83bab89f3\n",
            "  Stored in directory: /root/.cache/pip/wheels/9e/fc/1c/cf980e6413d3ee8e70cd8f39e2366b0f487e3e221aeb452eb0\n",
            "Successfully built ffmpy python-multipart\n",
            "Installing collected packages: rfc3986, pydub, ffmpy, websockets, uc-micro-py, sniffio, python-multipart, pycryptodome, orjson, mdurl, h11, aiofiles, uvicorn, markdown-it-py, linkify-it-py, anyio, starlette, mdit-py-plugins, httpcore, httpx, fastapi, gradio\n",
            "Successfully installed aiofiles-22.1.0 anyio-3.6.2 fastapi-0.89.1 ffmpy-0.3.0 gradio-3.16.2 h11-0.14.0 httpcore-0.16.3 httpx-0.23.3 linkify-it-py-1.0.3 markdown-it-py-2.1.0 mdit-py-plugins-0.3.3 mdurl-0.1.2 orjson-3.8.5 pycryptodome-3.16.0 pydub-0.25.1 python-multipart-0.0.5 rfc3986-1.5.0 sniffio-1.3.0 starlette-0.22.0 uc-micro-py-1.0.1 uvicorn-0.20.0 websockets-10.4\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 KB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing_extensions>=3.10.0.0 in /usr/local/lib/python3.8/dist-packages (from PyPDF2) (4.4.0)\n",
            "Installing collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting python-docx\n",
            "  Downloading python-docx-0.8.11.tar.gz (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: lxml>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from python-docx) (4.9.2)\n",
            "Building wheels for collected packages: python-docx\n",
            "  Building wheel for python-docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-docx: filename=python_docx-0.8.11-py3-none-any.whl size=184505 sha256=8766de4b8ac00beddad1237e7cbd85ec2fc229761d9c74ab74e0c181a4dd822c\n",
            "  Stored in directory: /root/.cache/pip/wheels/32/b8/b2/c4c2b95765e615fe139b0b17b5ea7c0e1b6519b0a9ec8fb34d\n",
            "Successfully built python-docx\n",
            "Installing collected packages: python-docx\n",
            "Successfully installed python-docx-0.8.11\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (1.3.5)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (1.21.6)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (2022.7)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "! pip install openai \n",
        "! pip install transformers \n",
        "! pip install gradio\n",
        "! pip install PyPDF2\n",
        "! pip install python-docx\n",
        "! pip install pandas\n",
        "\n",
        "# ! pip install openai --quiet\n",
        "# ! pip install transformers --quiet\n",
        "# ! pip install gradio --quiet\n",
        "# ! pip install PyPDF2 --quiet\n",
        "# ! pip install python-docx --quiet\n",
        "# ! pip install pandas --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        },
        "outputId": "5acb7753-9093-4d8a-f6df-d97023e886b7",
        "id": "KSt6VdJCGu4L"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Note: opening Chrome Inspector may crash demo inside Colab notebooks.\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, width, height, cache, element) => {\n",
              "                        if (!google.colab.kernel.accessAllowed && !cache) {\n",
              "                            return;\n",
              "                        }\n",
              "                        element.appendChild(document.createTextNode(''));\n",
              "                        const url = await google.colab.kernel.proxyPort(port, {cache});\n",
              "\n",
              "                        const external_link = document.createElement('div');\n",
              "                        external_link.innerHTML = `\n",
              "                            <div style=\"font-family: monospace; margin-bottom: 0.5rem\">\n",
              "                                Running on <a href=${new URL(path, url).toString()} target=\"_blank\">\n",
              "                                    https://localhost:${port}${path}\n",
              "                                </a>\n",
              "                            </div>\n",
              "                        `;\n",
              "                        element.appendChild(external_link);\n",
              "\n",
              "                        const iframe = document.createElement('iframe');\n",
              "                        iframe.src = new URL(path, url).toString();\n",
              "                        iframe.height = height;\n",
              "                        iframe.allow = \"autoplay; camera; microphone; clipboard-read; clipboard-write;\"\n",
              "                        iframe.width = width;\n",
              "                        iframe.style.border = 0;\n",
              "                        element.appendChild(iframe);\n",
              "                    })(7861, \"/\", \"100%\", 500, false, window.element)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "\"\"\"\n",
        "This program demonstrates how openAI's ChatGPT language model can be used to answer questions in specific domain areas. \n",
        "The program asks a user for a question in a prescribed domain area.  The program then compares the user's query against \n",
        "pre-loaded domain content to identify the most useful sections of content. The program answers the question by leveraging \n",
        "ChatGPT's powerful general capabilities with the newly incorporated domain knowledge.  Such an approach might be used, \n",
        "for example, to provide a customized chat box for an insurance company's customers, where the company's policy materials \n",
        "are brought in as domain content.  For this example, I compiled the 2023 investment outlook summaries posted on the websites of \n",
        "Morgan Stanley (https://www.morganstanley.com/ideas/global-investment-strategy-outlook-2023), \n",
        "JPMorgan (https://www.jpmorgan.com/insights/research/market-outlook) and \n",
        "Goldman Sachs (https://www.goldmansachs.com/insights/pages/gs-research/macro-outlook-2023-this-cycle-is-different/report.pdf).  \n",
        "Far more robust domain-specific responses are possible by further customizing/retraining ChatGPT.\n",
        "\"\"\"\n",
        "\n",
        "################################# LOAD LIBRARIES/IMPORTS #########################################\n",
        "\n",
        "! pip install openai --quiet\n",
        "! pip install transformers --quiet\n",
        "! pip install gradio --quiet\n",
        "! pip install PyPDF2 --quiet\n",
        "! pip install python-docx --quiet\n",
        "! pip install pandas --quiet\n",
        "\n",
        "\n",
        "import docx\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import openai\n",
        "import gradio as gr\n",
        "import pickle\n",
        "import os\n",
        "from transformers import GPT2TokenizerFast\n",
        "\n",
        "################################# VARIABLES #########################################\n",
        "\n",
        "USE_INTERFACE = True  # Change to False if you want to run the code without the Gradio interface, and instead see a single pre-supplied question \n",
        "filepath = '/content/drive/MyDrive/Colab Notebooks/Data/Compilation_investment_outlook_2023.docx' # Path to document containing domain content.  initial cleaning of domain content \n",
        "                                                                                                  # can be done inside (eg, using Python) or outside (eg, using Word),\n",
        "                                                                                                  # depending on needs and circumstances. \n",
        "# emb_filepath = 'PATH HERE'  # Path to document containing saved content embeddings, if applicable \n",
        "COMPLETIONS_MODEL = \"text-davinci-003\"  \n",
        "openai.api_key = 'YOUR KEY HERE'                                                   # TAKE OUT WHEN PUBLIC !!!!!!!!!\n",
        "# openai.api_key = os.environ[\"API_KEY\"] # Use if set as secret\n",
        "MODEL_NAME = \"curie\"\n",
        "DOC_EMBEDDINGS_MODEL = f\"text-search-{MODEL_NAME}-doc-001\"\n",
        "QUERY_EMBEDDINGS_MODEL = f\"text-search-{MODEL_NAME}-query-001\"\n",
        "MAX_SECTION_LEN =1100  # The API limits total tokens -- for the prompt containing the wuestion and domain-specific content and the answer -- to 2048 tokens, or about 1500 words.  \n",
        "SEPARATOR = \"\\n* \"  # A string called SEPARATOR is defined as the newline character followed by an asterisk and a space. This string will be used as a separator between different pieces of text.\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
        "separator_len = len(tokenizer.tokenize(SEPARATOR))\n",
        "COMPLETIONS_API_PARAMS = {\n",
        "    # We use temperature of 0.0 because it gives the most predictable, factual answer.\n",
        "    \"temperature\": 0.0,\n",
        "    \"max_tokens\": 300,\n",
        "    \"model\": COMPLETIONS_MODEL,\n",
        "}\n",
        "\n",
        "################################# FUNCTIONS #########################################\n",
        "\n",
        "def load_text(filepath):\n",
        "  \"\"\"\n",
        "  Loads a Microsoft Word document and returns a DataFrame containing the text of each paragraph in the document.\n",
        "\n",
        "  Input:\n",
        "    filepath (str): the filepath to the Microsoft Word document.\n",
        "    \n",
        "  Returns:\n",
        "    df (pandas.DataFrame): a DataFrame containing the 'content' column with the text of each paragraph in the document.\n",
        "  \"\"\"\n",
        "  # Open the Word document\n",
        "  doc = docx.Document(filepath)\n",
        "\n",
        "  # Create an empty pandas DataFrame\n",
        "  df = pd.DataFrame()\n",
        "\n",
        "  # Iterate through the paragraphs in the document and add each to the df\n",
        "  for i, p in enumerate(doc.paragraphs):\n",
        "\n",
        "      # Add the paragraph text [and index to the DataFrame]    \n",
        "      df.loc[i, 'content'] = p.text\n",
        "      # df.loc[i, 'paragraph_index'] = i\n",
        "\n",
        "  # Delete empty paragraphs\n",
        "  df['content'] = df['content'].replace('', np.nan)\n",
        "  df = df.dropna(axis=0, subset=['content']).reset_index(drop=True)\n",
        "\n",
        "  return df\n",
        "    \n",
        "def count_tokens(row):\n",
        "    \"\"\"count the number of tokens in a string\"\"\"\n",
        "    return len(tokenizer.encode(row))\n",
        "\n",
        "def truncate_text(df):\n",
        "    \"\"\"\n",
        "    Truncates the text in the 'content' column of the input DataFrame if the number of tokens \n",
        "    in the text exceeds a specified maximum number. It will set the truncated text and the \n",
        "    number of tokens in the 'content' and 'tokens' columns, respectively.\n",
        "\n",
        "    Input:\n",
        "    df (pandas.DataFrame): a DataFrame containing the 'content' column\n",
        "\n",
        "    Returns:\n",
        "    df (pandas.DataFrame): the input DataFrame with modified 'content' and 'tokens' columns.\n",
        "\n",
        "    \"\"\"\n",
        "    for i in range(len(df)):\n",
        "        if df['tokens'][i] > 590:\n",
        "            text = df['content'][i]\n",
        "            tokens = tokenizer.encode(text)\n",
        "            truncated_tokens = tokens[:590]\n",
        "            truncated_text = tokenizer.decode(truncated_tokens)\n",
        "            df.at[i, 'content'] = truncated_text\n",
        "            df.at[i, 'tokens'] = len(truncated_tokens)\n",
        "    return df\n",
        "\n",
        " \n",
        "def get_embedding(text, model): \n",
        "    \"\"\"\n",
        "    Generates an embedding for the given text using the specified OpenAI model.\n",
        "    \n",
        "    Args:\n",
        "        text (str): The text for which to generate an embedding.\n",
        "        model (str): The name of the OpenAI model to use for generating the embedding.\n",
        "    \n",
        "    Returns:\n",
        "        numpy.ndarray: The embedding for the given text.\n",
        "    \"\"\"\n",
        "    result = openai.Embedding.create(\n",
        "      model=model,\n",
        "      input=[text]\n",
        "    )\n",
        "    return result[\"data\"][0][\"embedding\"]\n",
        "\n",
        "def get_doc_embedding(text):\n",
        "    \"\"\"\n",
        "    Generates an embedding for the given text using the OpenAI document embeddings model.\n",
        "    \n",
        "    Args:\n",
        "        text (str): The text for which to generate an embedding.\n",
        "    \n",
        "    Returns:\n",
        "        numpy.ndarray: The embedding for the given text.\n",
        "    \"\"\"\n",
        "    return get_embedding(text, DOC_EMBEDDINGS_MODEL)\n",
        "\n",
        "def get_query_embedding(text):\n",
        "   \"\"\"\n",
        "    Generates an embedding for the given text using the OpenAI query embeddings model.\n",
        "    \n",
        "    Args:\n",
        "        text (str): The text for which to generate an embedding.\n",
        "    \n",
        "    Returns:\n",
        "        numpy.ndarray: The embedding for the given text.\n",
        "    \"\"\"\n",
        "   return get_embedding(text, QUERY_EMBEDDINGS_MODEL)\n",
        "\n",
        "def compute_doc_embeddings(df): \n",
        "     \"\"\"\n",
        "    Generate embeddings for each row in a Pandas DataFrame using the OpenAI document embeddings model.\n",
        "    \n",
        "    Args:\n",
        "        df (pandas.DataFrame): The DataFrame for which to generate embeddings.\n",
        "    \n",
        "    Returns:\n",
        "        dict: A dictionary that maps the embedding vectors to the indices of the rows that they correspond to.\n",
        "    \"\"\"\n",
        "     return {\n",
        "        idx: get_doc_embedding(r.content.replace(\"\\n\", \" \")) for idx, r in df.iterrows() # r here refers to each row \n",
        "   }\n",
        "\n",
        "def load_embeddings(fname): \n",
        "    \"\"\"\n",
        "    Load document embeddings and their keys from a CSV file.  Only if embeddings are pre-loaded.\n",
        "    \n",
        "    Args:\n",
        "        fname (str): The path to the CSV file. The file must have exactly these named columns: \n",
        "            \"title\", \"heading\", \"0\", \"1\", ... up to the length of the embedding vectors.\n",
        "    \n",
        "    Returns:\n",
        "        dict: A dictionary that maps the embedding vectors to tuples of the form (title, heading).\n",
        "    \"\"\"\n",
        "    \n",
        "    df = pd.read_csv(fname, header=0)\n",
        "    max_dim = max([int(c) for c in df.columns if c != \"title\" and c != \"heading\"])\n",
        "    return {\n",
        "           (r.title, r.heading): [r[str(i)] for i in range(max_dim + 1)] for _, r in df.iterrows()\n",
        "    }\n",
        "\n",
        "def vector_similarity(x, y):\n",
        "    \"\"\"\n",
        "    Calculate the similarity between two vectors using dot product.\n",
        "    \n",
        "    Args:\n",
        "        x (iterable): The first vector.\n",
        "        y (iterable): The second vector.\n",
        "    \n",
        "    Returns:\n",
        "        float: The dot product of the two vectors.\n",
        "    \"\"\"\n",
        "    return np.dot(np.array(x), np.array(y))\n",
        "\n",
        "def order_document_sections_by_query_similarity(query, contexts):\n",
        "  \"\"\"\n",
        "  Find the query embedding for the given query, and compare it against all of the pre-calculated document embeddings\n",
        "  to find the most relevant sections. \n",
        "   \n",
        "  Args:\n",
        "      query (str): The query for which to find relevant document sections.\n",
        "      contexts (dict): A dictionary mapping document embeddings to their indices.\n",
        "    \n",
        "  Returns:\n",
        "      list: A list of tuples, each containing the similarity score and index of a document section, sorted in descending\n",
        "      order of relevance.\n",
        "  \"\"\"\n",
        "  query_embedding = get_query_embedding(query)\n",
        "  document_similarities = sorted([(vector_similarity(query_embedding, doc_embedding), doc_index) for doc_index, doc_embedding in contexts.items()\n",
        "  ], reverse=True)\n",
        "    \n",
        "  return document_similarities\n",
        "    \n",
        "def construct_prompt(question, context_embeddings, df):\n",
        "    \"\"\"\n",
        "    Construct a prompt for answering a question using the most relevant document sections.\n",
        "    \n",
        "    Args:\n",
        "      question (str): The question to answer.\n",
        "      context_embeddings (dict): A dictionary mapping document embeddings to their indices.\n",
        "      df (pandas.DataFrame): A DataFrame containing the document sections.\n",
        "    \n",
        "    Returns:\n",
        "      str: The prompt, including the question and the relevant context.\n",
        "    \"\"\"\n",
        "    most_relevant_document_sections = order_document_sections_by_query_similarity(question, context_embeddings)\n",
        "    \n",
        "    chosen_sections = []\n",
        "    chosen_sections_len = 0\n",
        "    chosen_sections_indexes = []\n",
        "     \n",
        "    for _, section_index in most_relevant_document_sections:\n",
        "        # Add contexts until we run out of space.        \n",
        "        document_section = df.loc[section_index]\n",
        "        \n",
        "        chosen_sections_len += document_section.tokens + separator_len\n",
        "        if chosen_sections_len > MAX_SECTION_LEN:\n",
        "            break\n",
        "            \n",
        "        chosen_sections.append(SEPARATOR + document_section.content.replace(\"\\n\", \" \"))\n",
        "        chosen_sections_indexes.append(str(section_index))\n",
        "            \n",
        "    # Useful diagnostic information  -- FOR TESTING PURPOSES\n",
        "    print(f\"Selected {len(chosen_sections)} document sections:\")\n",
        "    print(\"\\n\".join(chosen_sections_indexes))\n",
        "    \n",
        "    header = \"\"\"Answer the question as truthfully as possible using the provided context, and if the answer is not contained within the text below, say \"Sorry, I don't know.\"\\n\\nContext:\\n\"\"\"\n",
        "\n",
        "    full_prompt = header + \"\".join(chosen_sections) + \"\\n\\n Q: \" + question + \"\\n A:\"\n",
        "\n",
        "    # print(full_prompt) # FOR TESTING PURPOSES\n",
        "\n",
        "    return full_prompt\n",
        "    \n",
        "\n",
        "def answer_query_with_context(\n",
        "    query,\n",
        "    df,\n",
        "    document_embeddings,\n",
        "    show_prompt: bool = False):\n",
        "    prompt = construct_prompt(\n",
        "        query,\n",
        "        document_embeddings,\n",
        "        df\n",
        "    )\n",
        "    \"\"\"\n",
        "    Answer a query using relevant context from a DataFrame.\n",
        "    \n",
        "    Args:\n",
        "        query (str): The query to answer.\n",
        "        df (pandas.DataFrame): A DataFrame containing the document sections.\n",
        "        document_embeddings (dict): A dictionary mapping document embeddings to their indices.\n",
        "        show_prompt (bool, optional): If `True`, print the prompt before generating a response.\n",
        "    \n",
        "    Returns:\n",
        "        str: The generated response to the query.\n",
        "    \"\"\"   \n",
        "    if show_prompt:\n",
        "        print(prompt)\n",
        "\n",
        "    response = openai.Completion.create(\n",
        "                prompt=prompt,\n",
        "                **COMPLETIONS_API_PARAMS\n",
        "            )\n",
        "\n",
        "    return response[\"choices\"][0][\"text\"].strip(\" \\n\")\n",
        "\n",
        "######################### MAIN PROGRAM #########################################\n",
        "\n",
        "# Load the text into dataframe \n",
        "df = load_text(filepath)\n",
        "# print(df.head()) # FOR TESTING PURPOSES\n",
        "\n",
        "# Count the tokens \n",
        "df = df.copy()    \n",
        "df['tokens'] = df['content'].apply(count_tokens)\n",
        "\n",
        "# print(df.head(10))   # FOR TESTING PURPOSES \n",
        "# print(df['content'][3])   # FOR TESTING PURPOSES\n",
        "\n",
        "# Call the truncate_text function on the dataframe  \n",
        "df = df.copy()    \n",
        "df = truncate_text(df)\n",
        "\n",
        "# print(df.head(10))  # FOR TESTING PURPOSES\n",
        "# print(df['content'][3])  # FOR TESTING PURPOSES\n",
        "\n",
        "#  Use code below only if importing embeddings from file,  rather than creating in real time through OpenAI API  \n",
        "# document_embeddings = load_embeddings(empb_filepath)  \n",
        "\n",
        "# Use code below if calculating the embeddings in real time via OpenAI API\n",
        "document_embeddings = compute_doc_embeddings(df[:33])  # Can limit size (eg, df[:10] if run into limit on free-of-charge usage\n",
        "\n",
        "# Embedding; embedding have 4096 dimensions, FOR TESTING ONLY\n",
        "# example_entry = list(document_embeddings.items())[4]\n",
        "# print(example_entry)\n",
        "# print (\"Length of example embedding =  \", len(example_entry[1]))\n",
        "\n",
        "if USE_INTERFACE:\n",
        "    demo = gr.Interface(\n",
        "    fn=lambda query: answer_query_with_context(query, df, document_embeddings),\n",
        "    inputs=gr.Textbox(lines=2,  label=\"Query\", placeholder=\"Type Question Here...\"),\n",
        "    outputs=gr.Textbox(lines=2, label=\"Answer\"),\n",
        "    description=\"Example of a domain-specific chatbot, using ChatGPT with supplemental content added.<br>\\\n",
        "                  Here, the content relates to the investment outlook for 2023, according to Morgan Stanley, JPMorgan and Goldman Sachs.<br>\\\n",
        "                  Sample queries: What is Goldman's outlook for inflation? What about the bond market? What does JPMorgan think about 2023?<br>\\\n",
        "                  NOTE: High-level demo only. Supplemental content used here limited to about 30 paragraphs, due to limits on free-of-charge usage of ChatGPT.<br>\\\n",
        "                  Far more robust domain-specific responses are possible.\",\n",
        "    title=\"Domain-Specific Chatbot\",)\n",
        "    # Launch the interface   \n",
        "    demo.launch()\n",
        "else:\n",
        "    prompt = construct_prompt(\n",
        "        'What is the outlook for inflation?',\n",
        "        document_embeddings,\n",
        "        df\n",
        "    )\n",
        "\n",
        "    # print(\"===\\n\", prompt) # FOR TESTING ONLY\n",
        "\n",
        "    answer_query_with_context(\"What is Goldman's outlook for inflation?\", df, document_embeddings)  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-3qGLQtXoop7"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1BAF6bD5qWorLdIOAO4HIIStErNc7WhWU",
      "authorship_tag": "ABX9TyOZvrKLQ+UQl/SwGNpp/6g9"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}